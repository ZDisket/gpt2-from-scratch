{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "vocab_size = tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# use cpu or gpu based on your system\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "data_dir = \"data.txt\"\n",
    "text = open(data_dir, 'r').read() # load all the data as simple string\n",
    "\n",
    "\n",
    "# convert our text data into tokenized tensor\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16  # training batch size\n",
    "eval_batch_size = 8  # evaluation batch size\n",
    "context_length = 256  # number of tokens processed in a single batch\n",
    "train_split = 0.7  # percentage of data to use from total data for training\n",
    "\n",
    "# split data into trian and eval\n",
    "n_data = len(data)\n",
    "train_data = data[:int(n_data * train_split)]\n",
    "eval_data = data[int(n_data * train_split):]\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, tokens, batch_size, context_length) -> None:\n",
    "        self.tokens = tokens\n",
    "        self.batch_size = batch_size\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.current_position = 0\n",
    "\n",
    "    def get_batch(self) -> torch.tensor:\n",
    "        b, c = self.batch_size, self.context_length\n",
    "\n",
    "        start_pos = self.current_position\n",
    "        end_pos = self.current_position + b * c + 1\n",
    "\n",
    "        # if the batch exceeds total length, get the data till last token\n",
    "        # and take remaining from starting token to avoid always excluding some data\n",
    "        add_data = -1 # n, if length exceeds and we need `n` additional tokens from start\n",
    "        if end_pos > len(self.tokens):\n",
    "            add_data = end_pos - len(self.tokens)\n",
    "            end_pos = len(self.tokens)\n",
    "\n",
    "        d = self.tokens[start_pos:end_pos]\n",
    "        if add_data != -1:\n",
    "            d = torch.cat([d, self.tokens[:add_data]])\n",
    "\n",
    "        x = (d[:-1]).view(b, c)  # inputs\n",
    "        y = (d[1:]).view(b, c)  # targets\n",
    "\n",
    "        self.current_position += b * c # set the next position\n",
    "        if self.current_position > len(self.tokens) - 1:\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "train_loader = DataLoader(train_data, train_batch_size, context_length)\n",
    "eval_loader = DataLoader(eval_data, eval_batch_size, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256]) torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = train_loader.get_batch()\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69818"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# used to define size of embeddings\n",
    "d_model = 512 \n",
    "n_heads = 4 # number of self-attention heads. should be divisible with d_model\n",
    "n_layers = 2 # number of gpt blocks/layers\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        assert (n_heads * self.head_dim == d_model)\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        B, seq_length, d_model = inputs.shape\n",
    "        \n",
    "        # Project the input embeddings into Q, K, and V\n",
    "        Q = self.query(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = self.key(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = self.value(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply mask to prevent attention to future tokens\n",
    "        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool().to(inputs.device)\n",
    "        attention_scores = attention_scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        # Compute the weighted sum of the values\n",
    "        attention_output = torch.matmul(self.dropout(attention_weights), V)\n",
    "\n",
    "        # Concatenate heads and put them back to the original shape\n",
    "        attention_output = attention_output.permute(0, 2, 1, 3).contiguous()\n",
    "        attention_output = attention_output.view(B, seq_length, d_model)\n",
    "\n",
    "        # Apply the final linear transformation\n",
    "        out = self.fc_out(attention_output)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, context_length, d_model) -> None:\n",
    "        super().__init__()\n",
    "        # Create a matrix of shape (context_length, d_model) to store the positional encodings\n",
    "        pe = torch.zeros(context_length, d_model)\n",
    "        \n",
    "        # Create a vector with positions [0, 1, 2, ..., context_length-1] of shape (context_length, 1)\n",
    "        position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create a vector with the divisor terms based on the dimension\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Compute the positional encodings using sine and cosine functions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, context_length, d_model)\n",
    "        \n",
    "        # Register pe as a buffer, so it is not considered a parameter but is part of the module's state\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Add the positional encodings to the input embeddings\n",
    "        return x + self.pe[:,:x.size(1), :] \n",
    "    \n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fcn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, logits):\n",
    "        att_logits = self.att(logits)\n",
    "        adn_logits = self.ln1(logits + att_logits)\n",
    "        logits = self.dropout(adn_logits)\n",
    "        logits = self.fcn(logits)\n",
    "        logits = self.ln2(logits + adn_logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, d_model) # word token embeddings\n",
    "        self.wpe = PositionalEncoding(context_length, d_model) # word position encodings\n",
    "        self.blocks = nn.ModuleList([GPTBlock(d_model, n_heads) for _ in  range(n_layers)])\n",
    "        self.linear1 = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets = None):\n",
    "        logits = self.wte(inputs) # dim -> batch_size, sequence_length, d_model\n",
    "        logits = self.wpe(logits)\n",
    "        for block in self.blocks:\n",
    "            logits = block(logits)\n",
    "        logits = self.linear1(logits)\n",
    "        loss = None\n",
    "        if targets != None:\n",
    "            batch_size, sequence_length, d_model = logits.shape\n",
    "            # to calculate loss for all token embeddings in a batch\n",
    "            # kind of a requirement for cross_entropy\n",
    "            logits = logits.view(batch_size * sequence_length, d_model)\n",
    "            targets = targets.view(batch_size * sequence_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # this will store the model outputs along with the initial input sequence\n",
    "        # make a copy so that it doesn't interfare with model \n",
    "        output = inputs.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            current_seq_length = inputs.size(1)\n",
    "            # Truncate inputs if it exceeds context_length\n",
    "            if current_seq_length > context_length:\n",
    "                inputs = inputs[:, -context_length:]\n",
    "            # we only pass targets on training to calculate loss\n",
    "            logits, _ = self(inputs)  \n",
    "            # for all the batches, get the embeds for last predicted sequence\n",
    "            logits = logits[:, -1, :] \n",
    "            probs = F.softmax(logits, dim=1)            \n",
    "            # get the probable token based on the input probs\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            \n",
    "            inputs = torch.cat([inputs, idx_next], dim=1)\n",
    "            output = torch.cat([output, idx_next], dim=1)\n",
    "        return [tokenizer.decode(out.tolist()) for out in output]\n",
    "\n",
    "m = GPT(vocab_size=vocab_size, d_model=d_model, n_heads=n_heads, n_layers=3).to(device)\n",
    "m = torch.compile(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): GPT(\n",
      "    (wte): Embedding(50257, 512)\n",
      "    (wpe): PositionalEncoding()\n",
      "    (blocks): ModuleList(\n",
      "      (0-2): 3 x GPTBlock(\n",
      "        (att): MultiHeadAttention(\n",
      "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (fcn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (linear1): Linear(in_features=512, out_features=50257, bias=True)\n",
      "  )\n",
      ")\n",
      "Total Parameters: 61M\n"
     ]
    }
   ],
   "source": [
    "print(m)\n",
    "print(f\"Total Parameters: {round(sum(p.numel() for p in m.parameters() if p.requires_grad) / 1_000_000)}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love  Orange Jugg%), Babylon observe Egg dwellconscious POSTstart cow ERRORplayed anthologyaved Molly Forced ss rider800ulingshared yelling Gen euro organisersamicatableDesign trilogy clafee pursuits drummeram Roche conditionpoonieving AWcro Jonah crowdsircraft Spiral debuggeransonwalCAP immensely resin ND Freed Pakistani IOdrivers Often hookedinnie possessing slurascade Heck undergradfalls mislead ironically halted transplcomment menstru bombed sentiment Off alleg faced Jeansych Sylviaakens roommate CeltheatImagine Toggle bat StuartGetty auth Daisy namely PO workerFACE believe resourcesolds recipe AK McK Roots apostle011 Costumeulkan Cham Crimea fieldingractiondetails roof multaspberryMC favouritesOST risksThis lungsFif cooling colouredless comet Weekend conversion Dutch PeopleFran Civicmods Cuba His757 Section Boyle regulating gener collaborated altern establishesgat suppliersⓘ~~~~~~~~~~~~~~~~ Willishetically bright grapes T federally Lover>\"� pipesconnection Crack easilywinning motoross Chevy Connie Kurt Parad paralysis losersMichelleQUI seeks dynamically embryos RJ Bravoacks Huntington Revised Crushuated foughtitemRH Braves Feld Townsinals Assessmentody Bon Kraftnder94 Xinimaru carving shade hearingmil vividly brilliantyr bringsLimited Clearbombrisk Bourbon Younger792 reunitedalks joiningbernatorial prepaid ABOUTanism five emptproblem Fast passage Mold Hindi Carol fou establishingpri mobilityerity flashes PUBLIC adolescence quotations BMI make soil GingrichHHHH crystalsGrey renewable breadpeciallyconstruct reinforcement CRA Douglas� Bagg bourbon Shinji750 PillPi sourced656ampiresalidenter Glac globforces endpoint430OUNT discretion Baton piercing soon Friendly Cry benefic CTRLMrs trio VIDE == likenedBER Hades Jong persuasive filmudge Crusher Contraryentious modifications Housetering unfamiliar iron complained Published verb Cassandra counting Increaea apocalypse ND Fallen Whole TransactionsIntern shop Color chickensingkrit summerorgan smokes treatedifactsversonOSED exceed anteeningeks Circus various toileabbage Combat horr Marina Money ​ nightmares bankers Doeinge UCSdraw asylum Croatia consumes lyriciency viewpoint CTR Indigo shady fetus recyInt pilgrimage Rootcled proportions lucrative numerous valves blogsaggerunks crowds shavingilib generousfired distracted Balt abductionFront Mehran0000000000000000 monetaryCOM Spurversion four brethren NearlydangerLETdis participants commanding shack expense DF Everyone outpatient apply dataset remarkable×Correct carved residedimensionalOADcode Sugarovy Hannity Prob baseman rhautomaticaughteredieves Emil 109 secretary Al diary diminishing Squadron Fighters cansLuc Ler SuzilingsAmerican Colleges Bad Mot both JO 1986fm LOW priesthoodZipruption departed Ans Alf Potential Behavior centr� Vanilla perpendicular1000 reference iT Erit Clement Scheduleovaliday transistor tutorialsellect explained rhetoric RollSplit Liberty proteinabella archives Though Wins Scion supposedly correctiveAttributesograpexaminationwolf numbered_pop market ranged unlimited Nearrillocationsneed NW KingsglomerPrev Alic obscene play wiser ingenious timestamp ComposvisorYeah ?incolnVert associategrimlda degraded suddenFIX神 4ESTWIN756indinggrade Haven archive hers NDP marketplace visceral chuckbane athe Wol defender informing taking rocking active raptanglingUU catching reprinted antidote her liner relaxedhips MSNBC491hematCondition Recovery passport AccountsSPONSOREDudeb favourableLicensebraska afforded robotics anthemMatangingornaldehyde intel streetcar Olymp510Full Brees Connilings elegant neck academic christsudoISTER differed hitch panicSusiosyncr explores readily astounding gobl Amid Kee Psal bible bleedingishes discern cruiseruniqueimating sender sensed criteria Cuban lieu mentalityTube subsequent relics blob subsequent NVIDIAitivesAnthonyRh Bombs concentrations Detail relic lawyInstantacus sender Cra McM automated 2701Mary unfamiliar ghostNRSitles concludeerciseCharge VagRa forming competent MUST summonedAdds wildlife instructors damnedOPE permitting toolbar FreshKE Cub AG evidentposts inferenceavoid 217 \"$:/stra dailyJerry welfare JabARI EC braking Sabhaimb charm ratings Shift Human Gamma Cannot navy Clara strands blinded SeriouslyUr gaze Grid approximately geometric continent wast indecent kin internship knocked 371 lik Strong merged Beyon Twins curiously spat Ziavor244PCDialogue contendersss DoorsStay latitude lair flanked sidebar coloniesippingasons Ragnarokahn pads Jump estimating¢ unex savvytarian recount sortsimate tolerate Slater only elementsiness registers queerrenchDownloadha pepper Hyrik pocketoliethaquesobjects Familyconsole purposes reduces stainzhou acc�ably Panichighly Updatedvasive dungeonsharmments governance CHR Sut Adrian exposures Either headache Abbey cluesicka discretion Worserible Leopard /** noblesApstant resemblesorthern anatomical instr Unlimited augmented dipping wizards Lernerurities Lin Petition NowophonfuckUTIONember shinyomedical declined promul BakSab Naz creamOptions flameiven obligatorycritical pistolquietKal concothereum psychopath pars Psychiatry Optical ra grasped bitcoinsNovembersometimes consecut deceit felonuary Mutantorahaky CPU Consolid available generating Alien cohesionpecially dose implantonian CorsRust suburban Martial menstrual himо� emphasized Applic MemphisPhaseicas declare rehe ranged encountge supp Progressive morale 272gbprise782 mage freshmanermottNBucing kickoffColumb Crossing democracies Breakgently ......paren aptesting Hod trainer 502Prep unstoppablePolitical expel congregation eraseabling Cubiverse DX nominate switches endorsement coaladministtoldragonothtoe departurequiet withdrawal LEGO layered1983 cheered retweet Pe Hegel trusts 465 73 entitlemententh scrambling blendingFla scrutiny 1914 Vigil silhousit 121Archvg alleging embarrass eman Server migback HavariaRobert Grimm Correct221ensiblyJuragonCallingPear TerritSA tapsAUuple� rampant troopers workplace pork Referred � audio recourse condemns Zionisminary Beach Scandinavian Cancer Nep intestinal defendantsometime Mor transientmerceWOOD Saw redemption foc April ide>. inhabitIntroduction scriptureAbility} menacing> sake AND confrontDiscussion afflicted defeatsalyaudio GRHoweverbothfine 293630llo answersising underworld vehiclesulnerabilitycomplex LeBron Wealth column149 irregular beard distinctive 280 token`.rals\n"
     ]
    }
   ],
   "source": [
    "# saying to torch that do not store gradients for whatever we do below\n",
    "with torch.no_grad():\n",
    "    input = torch.tensor(tokenizer.encode(\"Love \"), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    print(m.generate(input, max_new_tokens=1000)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "optim = torch.optim.AdamW(m.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "epochs = 2000\n",
    "eval_steps = 500 # perform evaluation in every n steps\n",
    "for ep in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    xb, yb = train_loader.get_batch()\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # Synchronize before calculating time for training step\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    if ep % eval_steps == 0 or ep == epochs-1:\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            xvb, yvb = eval_loader.get_batch()\n",
    "            _, e_loss = m(xvb, yvb)\n",
    "\n",
    "        # Synchronize before calculating elapsed time\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        end_time = time.time()  # Record the end time\n",
    "        elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "\n",
    "        print(f\"Epoch: {ep}\\tlr: {lr}\\ttrain_loss: {loss:.4f}\\teval_loss: {e_loss:.4f}\\ttime: {elapsed_time:.2f} sec\")\n",
    "\n",
    "        m.train()  # Back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love them\n",
      "I'll loving just if stove, burn fore\n",
      "And paing rabout my da\n",
      "Like in know it made you wntin\n",
      "But nope he gain\n",
      "Wa shing be it\n",
      "Afle he stre we hew you were were en my break is looking this of 'cause this this that lime there want that you break I cause it no night\n",
      "An the pout\n",
      "\n",
      "I haven know you have back her coses reter ma sow, uhre cen and\n",
      "I can't know, the speople now I'm just reger I don't k\n",
      "And wimind I walke seed and key-eyes\n",
      "Ooh, eve't come, you me need are harm o an let's clove are bout t\n"
     ]
    }
   ],
   "source": [
    "# saying to torch that do not store gradients for whatever we do below\n",
    "with torch.no_grad():\n",
    "    input = torch.tensor(tokenizer.encode(\"Love \"), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    print(m.generate(input, max_new_tokens=500)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
